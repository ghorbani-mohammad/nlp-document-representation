{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammad Ghorbani\\Anaconda\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\Mohammad Ghorbani\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Section 1\n",
    "\n",
    "import io, nltk, sys, gensim, math, collections, numpy\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading train file and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We preproceed our input and put it in puredInputSectionOne\n",
    "\n",
    "stop_words = [line.rstrip('\\n') for line in open('Stop_words2.txt', encoding=\"utf8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line.rstrip('\\n') for line in open('Data/HAM-Train.txt', encoding=\"utf8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "\n",
    "puredFile = open('puredInputSectionOne.txt', 'w', encoding=\"utf8\")\n",
    "for i, line in enumerate(lines):\n",
    "    delimiter = line.find('@@@@@@@@@@')+10\n",
    "    \n",
    "    true_labels.append(line[:delimiter-10])\n",
    "    \n",
    "    line = line[delimiter:]\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        else:\n",
    "            puredFile.write(token+\" \")\n",
    "    puredFile.write(\"\\n\")\n",
    "puredFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line.rstrip('\\n') for line in open('puredInputSectionOne.txt', encoding=\"utf8\")]\n",
    "sentences = LineSentence('puredInputSectionOne.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_vector = collections.defaultdict(lambda: 0)\n",
    "document_vector = []\n",
    "\n",
    "for i,line in enumerate(lines[:]):\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    sum_vector = model.wv[tokens[0]]\n",
    "    counter = 1\n",
    "    for token in tokens[1:]:\n",
    "        counter = counter + 1\n",
    "        sum_vector = sum_vector + model.wv[token]\n",
    "#     print(counter)\n",
    "#     print(sum_vector)\n",
    "#     print(sum_vector/counter)\n",
    "    document_vector.append(sum_vector/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 2649, 2: 1827, 3: 1582, 4: 1522, 0: 160})\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "kmeans.labels_[0]\n",
    "print(Counter(kmeans.labels_))\n",
    "print(Counter(kmeans.labels_)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = collections.defaultdict(lambda: 0)\n",
    "doc_type_counter = collections.defaultdict(lambda: 0)\n",
    "lines = [line.rstrip('\\n') for line in open('Data/HAM-Train.txt', encoding=\"utf8\")]\n",
    "for i,line in enumerate(lines):\n",
    "    delimiter = line.find('@@@@@@@@@@')\n",
    "    cluster[kmeans.labels_[i], line[:delimiter]] += 1\n",
    "    doc_type_counter[line[:delimiter]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_types = ['اقتصاد', 'سیاسی', 'اجتماعی', 'ادب و هنر', 'ورزش']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "   اقتصاد   160\n",
      "   سیاسی   0\n",
      "   اجتماعی   0\n",
      "   ادب و هنر   0\n",
      "   ورزش   0\n",
      "Cluster 1\n",
      "   اقتصاد   106\n",
      "   سیاسی   1611\n",
      "   اجتماعی   874\n",
      "   ادب و هنر   14\n",
      "   ورزش   44\n",
      "Cluster 2\n",
      "   اقتصاد   1628\n",
      "   سیاسی   84\n",
      "   اجتماعی   97\n",
      "   ادب و هنر   14\n",
      "   ورزش   4\n",
      "Cluster 3\n",
      "   اقتصاد   1\n",
      "   سیاسی   0\n",
      "   اجتماعی   3\n",
      "   ادب و هنر   7\n",
      "   ورزش   1571\n",
      "Cluster 4\n",
      "   اقتصاد   94\n",
      "   سیاسی   205\n",
      "   اجتماعی   663\n",
      "   ادب و هنر   407\n",
      "   ورزش   153\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Cluster \"+str(i))\n",
    "    for doc_type in doc_types:\n",
    "        print(\"   \"+doc_type+\"   \"+str(cluster[(i), doc_type]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Class entropy is: -2.1980033260728837\n",
      "Our Cluster entropy is: -2.0663232476449664\n",
      "Our Mutual Information is: -1.1959163121702119\n",
      "Our NMI is: 0.5608933985220335\n"
     ]
    }
   ],
   "source": [
    "# Calculating NMI\n",
    "\n",
    "total_documents = 7740\n",
    "\n",
    "\n",
    "# calculate class entropy\n",
    "y = 0\n",
    "for doc_type in doc_types: \n",
    "    x = doc_type_counter[doc_type]/total_documents\n",
    "#     print(x)\n",
    "#     print(x*math.log2(x))\n",
    "    y += (x*math.log2(x))\n",
    "class_entropy = y\n",
    "print(\"Our Class entropy is: \"+str(y))\n",
    "    \n",
    "    \n",
    "# calculating cluster entropy\n",
    "z = 0\n",
    "for i in range(5):\n",
    "    x = Counter(kmeans.labels_)[i]/total_documents\n",
    "    z += (x*math.log2(x))\n",
    "cluster_entropy = z\n",
    "print(\"Our Cluster entropy is: \"+str(z))\n",
    "\n",
    "\n",
    "# calculating mutual information\n",
    "k = 0\n",
    "for i in range(5):\n",
    "    l = 0\n",
    "    for doc_type in doc_types:\n",
    "        x = cluster[(i), doc_type] / Counter(kmeans.labels_)[i]\n",
    "        if (x!=0):\n",
    "            l += (x*math.log2(x))\n",
    "#         print(i)\n",
    "#         print(doc_type)\n",
    "#         print(cluster[(i), doc_type])\n",
    "#         print(Counter(kmeans.labels_)[i])\n",
    "    k += (Counter(kmeans.labels_)[i]/total_documents)*l\n",
    "mutual_information = y-k\n",
    "# print(k)\n",
    "print(\"Our Mutual Information is: \"+str(mutual_information))\n",
    "\n",
    "\n",
    "# calculate NMI\n",
    "n = (2 * mutual_information) / (class_entropy + cluster_entropy)\n",
    "print(\"Our NMI is: \"+str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7277777777777777\n"
     ]
    }
   ],
   "source": [
    "# Calculating Accuracy\n",
    "\n",
    "clusters_label = []\n",
    "for i in range(5):\n",
    "    max_val = 0 \n",
    "    for doc_type in doc_types:\n",
    "        if(max_val < cluster[(i), doc_type]):\n",
    "            max_val = cluster[(i), doc_type]\n",
    "            type = doc_type\n",
    "    clusters_label.append(type)\n",
    "#print(clusters_label)\n",
    "\n",
    "x = 0\n",
    "for i in range(5):\n",
    "    x += cluster[(i), clusters_label[i]]\n",
    "print(x/total_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating F-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736731024254575\n"
     ]
    }
   ],
   "source": [
    "# Calculating F-Measure\n",
    "\n",
    "\n",
    "# calculating precision & recall\n",
    "class_precision = []\n",
    "class_recall = []\n",
    "for i in range(5):\n",
    "    precision = cluster[(i), clusters_label[i]]/Counter(kmeans.labels_)[i]\n",
    "    recall = cluster[(i), clusters_label[i]]/doc_type_counter[clusters_label[i]]\n",
    "    class_precision.append(precision)\n",
    "    class_recall.append(recall)\n",
    "\n",
    "precision = 0\n",
    "recall = 0\n",
    "for i in range(5):\n",
    "    precision += (Counter(kmeans.labels_)[i]/total_documents)*class_precision[i]\n",
    "    recall += (Counter(kmeans.labels_)[i]/total_documents)*class_recall[i]\n",
    "\n",
    "# print(precision)\n",
    "# print(recall)\n",
    "\n",
    "f_measure = (2*precision*recall)/(precision+recall)\n",
    "print(f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.608154020385051, 0.8910782703886152, 0.993046776232617, 0.435611038107753]\n"
     ]
    }
   ],
   "source": [
    "print(class_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08044243338360986, 0.8478947368421053, 0.8185017596782302, 0.886568848758465, 0.40500916310323765]\n"
     ]
    }
   ],
   "source": [
    "print(class_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "for i in range(20):\n",
    "#     print(kmeans.labels_[i])\n",
    "    predicted_labels.append(kmeans.labels_[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 4, 1, 4, 1, 2, 3, 4, 2, 1, 3, 1, 1, 4, 2, 2, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['اقتصاد', 'سیاسی', 'ادب و هنر', 'سیاسی', 'اجتماعی', 'اجتماعی', 'اقتصاد', 'ورزش', 'ادب و هنر', 'اقتصاد', 'سیاسی', 'ورزش', 'سیاسی', 'ادب و هنر', 'اجتماعی', 'اقتصاد', 'اقتصاد', 'ورزش', 'اقتصاد', 'اجتماعی']\n"
     ]
    }
   ],
   "source": [
    "print(true_labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "for i, item in enumerate(clusters_label):\n",
    "#     print(i)\n",
    "#     print(item)\n",
    "    x[item] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'اقتصاد': 2, 'سیاسی': 1, 'ورزش': 3, 'اجتماعی': 4}\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_code = []\n",
    "for i, item in enumerate(true_labels[:]):\n",
    "    try:\n",
    "        true_labels_code.append(x[true_labels[i]])\n",
    "    except:\n",
    "        true_labels_code.append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740\n"
     ]
    }
   ],
   "source": [
    "print(len(true_labels_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our V-Measure is: 0.5608933985220332\n"
     ]
    }
   ],
   "source": [
    "# Calculating V-Measure\n",
    "print(\"Our V-Measure is: \"+str(v_measure_score(true_labels_code, kmeans.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1-2: Going to Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to test dataset\n",
    "\n",
    "lines_test = [line.rstrip('\\n') for line in open('Data/HAM-Test.txt', encoding=\"utf8\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_test = []\n",
    "doc_type_counter_test = collections.defaultdict(lambda: 0)\n",
    "puredFile = open('puredInputSectionOne_test.txt', 'w', encoding=\"utf8\")\n",
    "for i, line in enumerate(lines_test):\n",
    "    delimiter = line.find('@@@@@@@@@@')+10\n",
    "    doc_type_counter_test[line[:delimiter-10]] += 1\n",
    "    true_labels_test.append(line[:delimiter-10])\n",
    "    line = line[delimiter:]\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        else:\n",
    "            puredFile.write(token+\" \")\n",
    "    puredFile.write(\"\\n\")\n",
    "puredFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line.rstrip('\\n') for line in open('puredInputSectionOne_test.txt', encoding=\"utf8\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating document vector for test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating document vector for test documents\n",
    "document_vector_test = []\n",
    "\n",
    "for i,line in enumerate(lines[:]):\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    sum_vector = [0] * 300\n",
    "    counter = 1\n",
    "    for token in tokens[1:]:\n",
    "        counter = counter + 1\n",
    "        try:\n",
    "            sum_vector = sum_vector + model.wv[token]\n",
    "        except:\n",
    "            continue\n",
    "    document_vector_test.append(sum_vector/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_test_counter = collections.defaultdict(lambda: 0)\n",
    "test_clustering = collections.defaultdict(lambda: 0)\n",
    "predicted_labels_test = []\n",
    "\n",
    "for i, item in enumerate(document_vector_test[:]):\n",
    "    x = kmeans.predict(document_vector_test[i].reshape(1, -1))[0]\n",
    "    kmeans_test_counter[x] += 1\n",
    "    test_clustering[x, true_labels_test[i]] +=1\n",
    "    predicted_labels_test.append(x)\n",
    "    \n",
    "# print(test_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Class entropy is: -2.2116490392885453\n",
      "Our Cluster entropy is: -2.053538325758056\n",
      "Our Mutual Information is: -1.2401656563786483\n",
      "Our NMI is: 0.581529274208144\n"
     ]
    }
   ],
   "source": [
    "# Calculating NMI\n",
    "\n",
    "total_documents_test = len(document_vector_test)\n",
    "\n",
    "# calculate class entropy\n",
    "y = 0\n",
    "for doc_type in doc_types:\n",
    "    x = doc_type_counter_test[doc_type]/total_documents_test\n",
    "#     try:\n",
    "    y += (x*math.log2(x))\n",
    "#     except:\n",
    "#         continue\n",
    "class_entropy = y\n",
    "print(\"Our Class entropy is: \"+str(y))\n",
    "\n",
    "\n",
    "# calculating cluster entropy\n",
    "z = 0\n",
    "for i in range(5):\n",
    "    x = kmeans_test_counter[i]/total_documents_test\n",
    "    z += (x*math.log2(x))\n",
    "cluster_entropy = z\n",
    "print(\"Our Cluster entropy is: \"+str(z))\n",
    "    \n",
    "# calculating mutual information\n",
    "k = 0\n",
    "for i in range(5):\n",
    "    l = 0\n",
    "    for doc_type in doc_types:\n",
    "        x = test_clustering[i, doc_type] / kmeans_test_counter[i]\n",
    "        if (x!=0):\n",
    "            l += (x*math.log2(x))\n",
    "    k += (kmeans_test_counter[i]/total_documents_test)*l\n",
    "mutual_information = y-k\n",
    "# print(\"K is: \"+str(k))\n",
    "print(\"Our Mutual Information is: \"+str(mutual_information))\n",
    "\n",
    "# calculate NMI\n",
    "n = (2 * mutual_information) / (class_entropy + cluster_entropy)\n",
    "print(\"Our NMI is: \"+str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Accuracy\n",
    "\n",
    "clusters_label = []\n",
    "for i in range(5):\n",
    "    max_val = 0 \n",
    "    for doc_type in doc_types:\n",
    "        if(max_val < cluster[(i), doc_type]):\n",
    "            max_val = cluster[(i), doc_type]\n",
    "            type = doc_type\n",
    "    clusters_label.append(type)\n",
    "#print(clusters_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x0000020723E77E18>, {(4, 'ادب و هنر'): 53, (4, 'ورزش'): 21, (1, 'سیاسی'): 172, (1, 'اجتماعی'): 92, (2, 'اقتصاد'): 178, (3, 'ورزش'): 202, (2, 'اجتماعی'): 9, (4, 'اجتماعی'): 62, (1, 'ورزش'): 4, (2, 'سیاسی'): 14, (1, 'اقتصاد'): 11, (4, 'سیاسی'): 14, (0, 'اقتصاد'): 13, (4, 'اقتصاد'): 9, (3, 'ادب و هنر'): 2, (2, 'ورزش'): 1, (2, 'ادب و هنر'): 2, (1, 'ادب و هنر'): 1, (0, 'سیاسی'): 0, (0, 'اجتماعی'): 0, (0, 'ادب و هنر'): 0, (0, 'ورزش'): 0, (3, 'اقتصاد'): 0, (3, 'سیاسی'): 0, (3, 'اجتماعی'): 0})\n"
     ]
    }
   ],
   "source": [
    "print(test_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "   اقتصاد   13\n",
      "   سیاسی   0\n",
      "   اجتماعی   0\n",
      "   ادب و هنر   0\n",
      "   ورزش   0\n",
      "Cluster 1\n",
      "   اقتصاد   11\n",
      "   سیاسی   172\n",
      "   اجتماعی   92\n",
      "   ادب و هنر   1\n",
      "   ورزش   4\n",
      "Cluster 2\n",
      "   اقتصاد   178\n",
      "   سیاسی   14\n",
      "   اجتماعی   9\n",
      "   ادب و هنر   2\n",
      "   ورزش   1\n",
      "Cluster 3\n",
      "   اقتصاد   0\n",
      "   سیاسی   0\n",
      "   اجتماعی   0\n",
      "   ادب و هنر   2\n",
      "   ورزش   202\n",
      "Cluster 4\n",
      "   اقتصاد   9\n",
      "   سیاسی   14\n",
      "   اجتماعی   62\n",
      "   ادب و هنر   53\n",
      "   ورزش   21\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Cluster \"+str(i))\n",
    "    for doc_type in doc_types:\n",
    "        print(\"   \"+doc_type+\"   \"+str(test_clustering[(i), doc_type]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7290697674418605\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for i in range(5):\n",
    "#     print(test_clustering[(i), clusters_label[i]])\n",
    "    x += test_clustering[(i), clusters_label[i]]\n",
    "print(x/total_documents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating F-Measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7485153758376375\n"
     ]
    }
   ],
   "source": [
    "# Calculating F-Measure\n",
    "\n",
    "\n",
    "# calculating precision & recall\n",
    "class_precision_test = []\n",
    "class_recall_test = []\n",
    "for i in range(5):\n",
    "    precision = test_clustering[(i), clusters_label[i]]/kmeans_test_counter[i]\n",
    "    recall = test_clustering[(i), clusters_label[i]]/doc_type_counter_test[clusters_label[i]]\n",
    "    class_precision.append(precision)\n",
    "    class_recall.append(recall)\n",
    "\n",
    "precision = 0\n",
    "recall = 0\n",
    "for i in range(5):\n",
    "    precision += (kmeans_test_counter[i]/total_documents_test)*class_precision[i]\n",
    "    recall += (kmeans_test_counter[i]/total_documents_test)*class_recall[i]\n",
    "\n",
    "# print(precision)\n",
    "# print(recall)\n",
    "\n",
    "f_measure = (2*precision*recall)/(precision+recall)\n",
    "print(f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "for i, item in enumerate(clusters_label):\n",
    "#     print(i)\n",
    "#     print(item)\n",
    "    x[item] = i\n",
    "\n",
    "true_labels_code_test = []\n",
    "for i, item in enumerate(true_labels_test[:]):\n",
    "    try:\n",
    "        true_labels_code_test.append(x[true_labels_test[i]])\n",
    "    except:\n",
    "        true_labels_code_test.append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our V-Measure is: 0.5815292742081438\n"
     ]
    }
   ],
   "source": [
    "# Calculating V-Measure\n",
    "print(\"Our V-Measure is: \"+str(v_measure_score(true_labels_code_test, predicted_labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
